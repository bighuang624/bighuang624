üëã Hi! I am Siteng Huang (ÈªÑÊÄùËÖæ in Chinese). I work at <a href="https://damo.alibaba.com/" target="_blank"><img src='./images/damo.png' align="center" style='vertical-align: middle; width: 19px;'></a> [DAMO Academy](https://damo.alibaba.com/), Alibaba Group, as an Algorithm Expert through the [AliStar program](https://talent.alibaba.com/activity/ali-star?lang=zh). I received my Ph.D. degree from <a href="http://www.zju.edu.cn/" target="_blank"><img src='./images/zju.png' align="center" style='vertical-align: middle; width: 19px;'></a> [Zhejiang University](http://www.zju.edu.cn/) in June 2024, affiliated with a joint program with <a href="https://www.westlake.edu.cn/" target="_blank"><img src='./images/westlake.png' align="center" style='vertical-align: middle; width: 19px;'></a> [Westlake University](https://www.westlake.edu.cn/) at [Machine Intelligence Laboratory (MiLAB)](https://milab.westlake.edu.cn/) and advised by Prof. [Donglin Wang](https://en.westlake.edu.cn/faculty/donglin-wang.html). In my Ph.D. study, I also spent wonderful internship time at <a href="https://tongyi.aliyun.com/" target="_blank"><img src='./images/tongyi.png' align="center" style='vertical-align: middle; width: 19px;'></a> [TongYi Lab](https://tongyi.aliyun.com/), Alibaba Group. Before that, I received my B.Eng. Degree from School of Computer Science, <a href="https://www.whu.edu.cn/" target="_blank"><img src='./images/whu.png' align="center" style='vertical-align: middle; width: 19px;'></a> [Wuhan University](https://www.whu.edu.cn/) in June 2019.

üî¨ My research has centered on the **perception, understanding, reasoning, and generation of multimodal (including images, videos, language, dynamics, etc.) data from both the internet and the physical world**. I also focus on **efficientAI** (in terms of data, time, parameters, memory, etc.) for multimodal applications. I have published 30+ papers <a class='all_citation_badges' href="" target="_blank"></a> on the above topics at the top-tier international AI conferences and journals. Recently, I devote myself to the development of multi-modal generative, embodied, and unified foundation models.
<!-- <a href="https://scholar.google.com/citations?user=mhpkWSYAAAAJ" target="_blank"><img src="https://img.shields.io/badge/dynamic/json?label=Paper%20Citations&query=total_citations&url=https%3A%2F%2Fcse.bth.se%2F~fer%2Fgooglescholar-api%2Fgooglescholar.php%3Fuser%3DmhpkWSYAAAAJ&logo=googlescholar&style=social" align="center" alt="Google Scholar"></a> -->

<!-- <a href='https://damo.alibaba.com/' target="_blank"><img src='./images/alibaba.png' align="center" style='vertical-align: middle; width: 78px;'></a> -->

Welcome to refer to my full publication list at [my personal homepage](https://kyonhuang.top/#-publications).
<p align="center"><a href="https://twitter.com/KyonHuang" target="_blank"><img src="https://img.shields.io/twitter/follow/KyonHuang.svg?style=social" alt="Twitter"></a> <a href="https://github.com/bighuang624?tab=followers" target="_blank"><img src="https://img.shields.io/github/followers/bighuang624.svg?label=Follow%20@bighuang624&style=social" alt="GitHub"></a> <a href="https://github.com/bighuang624" target="_blank"><img src="https://img.shields.io/github/stars/bighuang624.svg?style=social" alt="GitHub"></a></p>

### üì¢ News

<!-- ÂèÇËÄÉ https://huanwang.tech/ ÁöÑÊ†∑Âºè -->

* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2026/02/21</span> **[CVPR'26]** 2 papers ([HiF-VLA](https://arxiv.org/abs/2512.09928) and [V¬≤Drop](https://arxiv.org/abs/2509.01552)) got accepted for CVPR 2026 (Main Conference)! 
* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2026/02/18</span> **[RA-L]** [RoboSimGS](https://arxiv.org/abs/2510.10637), a novel Real2Sim2Real framework that converts multi-view real-world images into scalable, high-fidelity, and physically interactive simulation environments for robotic manipulation, got accepted for RA-L! See [Project page](https://robosimgs.github.io/) for the overview video!
* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2026/02/10</span> **[RynnBrain]** We presented [RynnBrain](https://alibaba-damo-academy.github.io/RynnBrain.github.io/), an embodied foundation model grounded in physical reality, including dense (2B, 8B) and MoE (30B) variants, alongside three specialized models: RynnBrain‚ÄëPlan (manipulation planning), RynnBrain‚ÄëNav (navigation), and RynnBrain‚ÄëCoP (spatial reasoning). See [Github](https://github.com/alibaba-damo-academy/RynnBrain) and [Chinese report from Êú∫Âô®‰πãÂøÉ](https://mp.weixin.qq.com/s/53UMfJL6VG-TAA4KJNv8Mg). <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2026/02/19</span> We released the [technical report](https://arxiv.org/pdf/2602.14979)!
* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2026/01/31</span> **[ICRA'26]** [RynnVLA-001](https://arxiv.org/abs/2509.15212), the VLA foundation model, got accepted for ICRA 2026!
* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2026/01/22</span> **[Talk]** I gave a talk titled *Physical AI Ecosystem: Tackling the Key Barriers to Embodied Intelligence* in [AAAI-26 Interactive Industry Sessions](https://aaai.org/conference/aaai/aaai-26/interactive-industry-sessions/#Alibaba).
* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2025/12/11</span> **[Preprint]** We released [HiF-VLA](https://arxiv.org/abs/2512.09928) (Hindsight, Insight, and Foresight for VLAs), a unified framework that encodes past dynamics through hindsight priors, anticipates future motion via foresight reasoning, and integrates both through a hindsight-modulated joint expert to enable a "**think-while-acting**" paradigm for long-horizon manipulation! [Project page](https://hifvla.github.io) and [Code](https://github.com/OpenHelix-Team/HiF-VLA) are available! <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2026/02/21</span> HiF-VLA got accepted for CVPR 2026!
* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2025/11/24</span> **[Preprint]** We released [RynnVLA-002](https://arxiv.org/abs/2511.17502), an upgraded version of WorldVLA, a more powerful VLA and world model unified model! Get videos and code at [Github](https://github.com/alibaba-damo-academy/RynnVLA-002)!
* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2025/11/08</span> **[AAAI'26]** 4 papers got accepted for AAAI 2026! They included training-free MLLM inference acceleration methods [FiCoCo](https://arxiv.org/abs/2411.17686) and [GlobalCom<sup>2</sup>](https://arxiv.org/abs/2501.05179), dexterous grasping policy [AffordDex](https://arxiv.org/abs/2508.08896), and tiny-scale VLA [VLA-Adapter](https://arxiv.org/abs/2509.09372).
* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2025/09/19</span> **[NeurIPS'25]** [SSR](https://arxiv.org/abs/2505.12448) got accepted for NeurIPS 2025! The work transforms raw depth data into structured, interpretable textual CoT, enhancing spatial reasoning capabilities of MLLMs. See [Project page](https://yliu-cs.github.io/SSR/) and [Github](https://github.com/yliu-cs/SSR)!
* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2025/09/12</span> **[Preprint]** We released [VLA-Adapter](https://arxiv.org/abs/2509.09372), which reduces reliance on large-scale VLMs and extensive pre-training by using a lightweight Policy module with Bridge Attention, achieving SOTA performance and fast inference speed with minimal computational resources! [Checkpoint](https://huggingface.co/VLA-Adapter) has been available! See [Project page](https://vla-adapter.github.io/) for more details. Got [#1 Paper of the day](https://huggingface.co/papers/2509.09372) on huggingface papers! <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2025/11/08</span> VLA-Adapter got accepted for AAAI 2026 Oral!
* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2025/08/13</span> **[Preprint]** We released [AffordDex](https://arxiv.org/abs/2508.08896), a universal grasping policy for dexterous hands with an inherent understanding of both motion priors and object affordances! Grasping videos can be found in [Project page](https://afforddex.github.io/)! <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2025/11/08</span> AffordDex got accepted for AAAI 2026!
* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2025/08/08</span> **[DAMO RynnBot]** We open-sourced [RynnEC](https://github.com/alibaba-damo-academy/RynnEC): a video MLLM for embodied cognition tasks, [RynnVLA-001](https://github.com/alibaba-damo-academy/RynnVLA-001): a VLA model based on pretrained video generation model, [RynnRCP](https://github.com/alibaba-damo-academy/RynnRCP): a complete set of robot service agreements and frameworks! <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2025/08/11</span> We released the [technical blog](https://huggingface.co/blog/Alibaba-DAMO-Academy/rynnvla-001) for RynnVLA-001! <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2025/09/19</span> We released the [technical report](https://arxiv.org/abs/2509.15212) for RynnVLA-001!
* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2025/08/02</span> **[CoRL'25]** [Long-VLA](https://arxiv.org/abs/2508.19958), a novel framework designed to enhance VLA models for challenging **long-horizon** robotic manipulation tasks, got accepted for CoRL 2025!
* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2025/07/24</span> **[DAMO RynnBot]** We released [RynnBot PlayGround Beta](https://developer.damo-academy.com/playground), a platform that provides data management, SOTA VLA models, model training and validation, cloud-edge collaborative deployment, and so on! Welcome to follow our continuous progress!
* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2025/06/27</span> **[Preprint]** We released [WorldVLA](https://arxiv.org/abs/2506.21539), an autoregressive action world model that unifies action and image understanding and generation! [Code](https://github.com/alibaba-damo-academy/WorldVLA) has been available!
* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2025/06/26</span> **[ICCV'25]** [CARP](https://arxiv.org/abs/2412.06782), **C**oarse-to-fine **A**uto**R**egressive **P**rediction for visuomotor policy learning, got accepted for ICCV 2025! The approach produces highly accurate and smooth robot actions, achieving up to a 10% improvement of success rates, and delivers 10x faster inference compared to state-of-the-art policies. Paper, code and cool videos can be found in [Project page](https://carp-robot.github.io/)!
* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2025/05/22</span> **[Preprint]** We released [VARD](https://arxiv.org/abs/2505.15791), a novel RL fine-tuning method on diffusion-based generative models for both protein structure and text-to-image synthesis, enhancing sample quality with improved efficiency, effective mitigation of reward hacking, and broad applicability.
* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2025/05/07</span> **[Preprint]** We released [OpenHelix](https://arxiv.org/abs/2505.03912), a low-cost open-source dual-system VLA with systematic empirical evaluations on the core design elements. [Code](https://github.com/OpenHelix-robot/OpenHelix/) and [List of papers](https://github.com/OpenHelix-robot/awesome-dual-system-vla/) have been available!
* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2025/03/31</span> **[Preprint]** We released [Unicorn](https://arxiv.org/abs/2503.22655) to explore the question: can high-quality multimodal training data be synthesized purely from text?
* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2025/03/28</span> **[Survey Preprint]** We released [Exploring the Evolution of Physics Cognition in Video Generation: A Survey](https://arxiv.org/abs/2503.21765), which dives deep into the development of physics cognition in video generation, from basic perception to active cognition! [List of papers](https://github.com/minnie-lin/Awesome-Physics-Cognition-based-Video-Generation) has been available!
* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2025/03/11</span> **[TCSVT'25]** [M2IST](https://arxiv.org/abs/2407.01131), a novel Multi-Modal Interactive Side-Tuning method that effectively addresses the challenges of insufficient multi-modal interaction and high GPU memory consumption, got accepted for IEEE Transactions on Circuits and Systems for Video Technology! [Code](https://github.com/xuyang-liu16/M2IST) has been available!
* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2025/02/24</span> **[Preprint]** We released [Humanoid-VLA](https://arxiv.org/abs/2502.14795), a novel framework that integrates language understanding, egocentric scene perception, and motion control, enabling universal humanoid control!
* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2025/01/28</span> **[ICRA'25]** [QUART-Online](https://arxiv.org/abs/2412.15576), a novel latency-free quadruped MLLM model that achieves real-time inference while boosting the success rate across various tasks by 65%, got accepted for ICRA 2025! See [Project page](https://quart-online.github.io/).
* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2025/01/23</span> **[ICLR'25]** [ToCa](https://arxiv.org/abs/2410.05317), a token-wise feature caching method that achieves a 2x acceleration for PixArt-Œ±, OpenSora, and DiT while maintaining nearly lossless generation quality, got accepted for ICLR 2025! [Code](https://github.com/Shenyi-Z/ToCa) has been available!
* <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2025/01/10</span> **[Preprint]** We released [GlobalCom<sup>2</sup>](https://arxiv.org/abs/2501.05179), a "global-to-local" approach for training-free acceleration of high-resolution MLLMs with AnyRes strategy. [Code](https://github.com/xuyang-liu16/GlobalCom2) has been available! <span style="font-size:12px;color:#FFFFFF;background-color:#007ec6;padding:1px 5px 1.5px 5px;">2025/11/08</span> GlobalCom<sup>2</sup> got accepted for AAAI 2026!